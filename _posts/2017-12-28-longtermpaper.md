---
layout: post
title: "The expected value of the long-term future"
date: 2017-12-28 23:39
---

I wrote an article describing a simple model of the long-term future. Here it is:
* in [PDF](/ltf-paper/longtermfuture.pdf) format
* in [tex](/ltf-paper/longtermfuture.tex) format

Considerate feedback is appreciated. The more specific you can be, the better, and feedback before next week would be most valuable. You can write your feedback publicly on the EA forum or LessWrong, or you can annotate the files and email them to me at [tmksitt@gmail.com](mailto:tmksitt@gmail.com). Thanks!

Summary:
> A number of ambitious arguments have recently been proposed about the moral importance of the long-term future of humanity, on the scale of millions and billions of years. Several people have advanced arguments for a cluster of related views. Authors have variously claimed that shaping the trajectory along which our descendants develop over the very long run (Beckstead, 2013), or reducing extinction risk, or minimising existential risk (Bostrom, 2002), or reducing risks of severe suffering in the long-term future (Althaus and Gloor, 2016) are of huge or overwhelming importance. In this paper, I develop a simple model of the value of the long-term future, from a totalist, consequentialist, and welfarist (but not necessarily utilitarian) point of view. I show how the various claims can be expressed within the model, clarifying under which conditions the long-term becomes overwhelmingly important, and drawing tentative policy implications.

<hr> <!-- hr to be added before footnotes--> 